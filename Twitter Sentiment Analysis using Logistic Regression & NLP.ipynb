{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from os import getcwd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download(\"twitter_samples\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load dataset\n",
    "all_pos_twts = twitter_samples.strings('positive_tweets.json')\n",
    "all_neg_twts = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"Cleans and preprocesses a tweet.\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words(\"english\")\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    return [stemmer.stem(word) for word in tweet_tokens if word not in stopwords_english and word not in string.punctuation]\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Builds a frequency dictionary mapping (word, sentiment) pairs to their counts.\"\"\"\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            freqs[pair] = freqs.get(pair, 0) + 1\n",
    "    return freqs\n",
    "\n",
    "def extract_features(tweet, freqs):\n",
    "    \"\"\"Extracts features for a given tweet based on word frequencies.\"\"\"\n",
    "    word_l = process_tweet(tweet)\n",
    "    x = np.zeros((1, 3))\n",
    "    x[0, 0] = 1  # Bias term\n",
    "    for word in word_l:\n",
    "        x[0, 1] += freqs.get((word, 1.0), 0)\n",
    "        x[0, 2] += freqs.get((word, 0.0), 0)\n",
    "    return x\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def gradient_descent(x, y, theta, alpha, num_iter):\n",
    "    \"\"\"Performs gradient descent to optimize theta.\"\"\"\n",
    "    m = x.shape[0]\n",
    "    for _ in range(num_iter):\n",
    "        z = np.dot(x, theta)\n",
    "        h = sigmoid(z)\n",
    "        J = (-1/m) * (np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)))\n",
    "        theta -= (alpha/m) * (np.dot(x.T, (h - y)))\n",
    "    return float(J), theta\n",
    "\n",
    "# Prepare training and testing data\n",
    "labels = np.append(np.ones(len(all_pos_twts)), np.zeros(len(all_neg_twts)))\n",
    "train_x, test_x = all_pos_twts[:4000] + all_neg_twts[:4000], all_pos_twts[4000:] + all_neg_twts[4000:]\n",
    "train_y, test_y = np.append(np.ones((len(train_x)//2, 1)), np.zeros((len(train_x)//2, 1)), axis=0), np.append(np.ones((len(test_x)//2, 1)), np.zeros((len(test_x)//2, 1)), axis=0)\n",
    "\n",
    "# Build frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "# Train logistic regression model\n",
    "X = np.array([extract_features(tweet, freqs) for tweet in train_x]).reshape(len(train_x), 3)\n",
    "Y = train_y\n",
    "J, theta = gradient_descent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "\n",
    "def predict_tweet(tweet, freqs, theta):\n",
    "    \"\"\"Predicts the sentiment of a tweet.\"\"\"\n",
    "    x = extract_features(tweet, freqs)\n",
    "    return sigmoid(np.dot(x, theta))\n",
    "\n",
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\"Evaluates model accuracy on test data.\"\"\"\n",
    "    y_hat = [1 if predict_tweet(tweet, freqs, theta) > 0.5 else 0 for tweet in test_x]\n",
    "    return (np.array(y_hat) == np.squeeze(test_y)).sum() / len(test_y)\n",
    "\n",
    "# Evaluate model accuracy\n",
    "accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {accuracy:.4f}\")\n",
    "\n",
    "# Test on a sample tweet\n",
    "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
    "print(f\"Processed Tweet: {process_tweet(my_tweet)}\")\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print('Positive sentiment' if y_hat > 0.5 else 'Negative sentiment')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
